{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lIYdn1woOS1n"},"outputs":[],"source":["#《AI数字人原理与实现》方进 著 源代码"]},{"cell_type":"code","source":["#第3章　数字人视觉算法"],"metadata":{"id":"yoNSIPkVG5up"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.2.1　表情识别\n","#1．静态图像表情识别\n","#（1）支持向量机\n","#使用SVM进行静态表情识别的Python代码如下\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import StandardScaler\n","# 加载数据集（请替换为你的数据集路径）\n","data = np.load(\"your_dataset.npz\")\n","X = data[\"X\"]\n","y = data[\"y\"]\n","# 划分训练集和测试集\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# 数据预处理\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","# SVM模型训练\n","svm_model = SVC(kernel=\"rbf\", C=1, gamma=\"scale\")\n","svm_model.fit(X_train, y_train)\n","# 预测\n","y_pred = svm_model.predict(X_test)\n","# 输出分类报告\n","print(classification_report(y_test, y_pred))"],"metadata":{"id":"-q09YcWUHMxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#（2）深度学习方法\n","#使用 CNN 进行静态表情识别的 Python 代码如下。\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.python.keras.utils import load_img, img_to_array\n","from tensorflow.python.keras.utils import to_categorical\n","from tensorflow.python.keras.models import model_from_json\n","# 加载数据集（请替换为你的数据集路径）\n","data = np.load(\"your_dataset.npz\")\n","X = data[\"X\"]\n","y = data[\"y\"]\n","# 划分训练集和测试集\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# 数据预处理\n","X_train = np.array([img_to_array(load_img(img, target_size=(48, 48))) for img in X_train])\n","X_test = np.array([img_to_array(load_img(img, target_size=(48, 48))) for img in X_test])\n","# 转换为张量并归一化\n","X_train = np.array(X_train) / 255.0\n","X_test = np.array(X_test) / 255.0\n","#将标签转换为独热编码\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","# 构建CNN模型\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(<num_classes>, activation='softmax')) # num_classes为类别数量\n","# 编译模型\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# 训练模型\n","model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n","# 保存模型\n","model.save(\"cnn_model.h5\")\n","# 加载模型进行预测\n","loaded_model = model_from_json(open(\"cnn_model.json\", \"r\").read())\n","loaded_model.load_weights(\"cnn_model.h5\")\n","predictions = loaded_model.predict(X_test)\n","# 输出预测结果\n","print(predictions)"],"metadata":{"id":"zqKy5_mmIZgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -V\n","import tensorflow as tf\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"oRqm3-zdwkFk","executionInfo":{"status":"ok","timestamp":1723369202144,"user_tz":-480,"elapsed":385,"user":{"displayName":"方进","userId":"17751863592866608639"}},"outputId":"9b987516-679e-4b9f-94f5-5fb56b0a00dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]},{"output_type":"execute_result","data":{"text/plain":["'2.17.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["#2．序列图像表情识别\n","#下面是一个简化的LSTM算法的例子，展示了使用keras库创建一个简单的序列图像表情识别模型的方法。\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","# 构建LSTM模型\n","model = keras.Sequential([\n"," layers.LSTM(64, input_shape=(seq_len, feature_dim), return_sequences=True),\n"," layers.LSTM(64),\n"," layers.Dense(num_classes, activation='softmax')\n","])\n","\n","# 编译模型\n","model.compile(optimizer='adam',\n"," loss='categorical_crossentropy',\n"," metrics=['accuracy'])\n","# 准备训练数据和标签\n","X_train = ...\n","y_train = ...\n","# 训练模型\n","model.fit(X_train, y_train, epochs=10, batch_size=32)\n","# 准备测试数据\n","X_test = ...\n","# 进行推理\n","predictions = model.predict(X_test)\n","# 获取预测结果\n","predicted_emotion = predictions.argmax(axis=1)"],"metadata":{"id":"OI8prTXRLgPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．多模态表情识别\n","#我们将使用Python的keras库来构建一个简单的多模态表情识别模型。\n","#请注意，这个示例仅用于演示目的，需要根据自己的实际数据集进行调整。\n","import numpy as np\n","import pandas as pd\n","from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras.layers import Input, Dense, Dropout, Concatenate, TimeDistributed, Activation\n","from tensorflow.python.keras.utils import load_img, img_to_array\n","from tensorflow.python.keras.utils import pad_sequences\n","from tensorflow.python.keras.utils import to_categorical\n","from tensorflow.python.keras.callbacks import ModelCheckpoint\n","# 加载数据集（请替换为你的数据集路径）\n","data = pd.read_csv(\"your_dataset.csv\")\n","# 提取视觉模态（人脸关键点）特征\n","X_visual = data[[\"keypoint_x1\", \"keypoint_y1\", ...]].values\n","# 提取音频模态（语音信号）特征\n","X_audio = data[[\"audio_feature1\", \"audio_feature2\", ...]].values\n","# 提取文本模态（文本描述）特征\n","X_text = data[\"text_description\"]\n","# 将标签转换为独热编码\n","y = to_categorical(data[\"expression\"])\n","# 数据预处理:将文本描述扩充到固定长度（结尾补0）\n","max_length = 100 # 设定最大文本长度\n","X_text = pad_sequences([X_text], maxlen=max_length, padding='post')\n","# 构建多模态模型\n","input_visual = Input(shape=(X_visual.shape[1],))\n","input_audio = Input(shape=(X_audio.shape[1],))\n","input_text = Input(shape=(max_length,))\n","x_visual = TimeDistributed(Dense(64, activation='relu'))(input_visual)\n","x_audio = Dense(64, activation='relu')(input_audio)\n","x_text = Dense(64, activation='relu')(input_text)\n","merged = Concatenate()([x_visual, x_audio, x_text])\n","model = Model(inputs=[input_visual, input_audio, input_text], outputs=merged)\n","# 添加全连接层和输出层\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(<num_classes>, activation='softmax')) # num_classes为类别数量\n","# 编译模型\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# 训练模型\n","checkpointer = ModelCheckpoint(filepath=\"multimodal_model.h5\", verbose=1, save_best_only=True)\n","model.fit([X_visual, X_audio, X_text], y, batch_size=32, epochs=10, validation_split=0.2,\n","callbacks=[checkpointer])\n","# 加载最佳模型\n","model.load_weights(\"multimodal_model.h5\")\n","# 推理示例\n","new_visual_data = np.array([[new_visual_data1, new_visual_data2, ...]])\n","new_audio_data = np.array([[new_audio_data1, new_audio_data2, ...]])\n","new_text_data = np.array([new_text_description])\n","new_text_data_padded = pad_sequences([new_text_data], maxlen=max_length, padding='post')\n","predictions = model.predict([new_visual_data, new_audio_data, new_text_data_padded])\n","predicted_class = np.argmax(predictions, axis=1)"],"metadata":{"id":"1QO0tOVNap3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.2.2　表情生成\n","#1．基于GAN的表情生成\n","#CGAN Python 代码示例如下。\n","import tensorflow as tf\n","from tensorflow.python.keras.layers import Input, Dense, Reshape, Flatten, Concatenate, LeakyReLU\n","from tensorflow.python.keras.layers import BatchNormalization, Activation, Embedding, multiply\n","from tensorflow.python.keras.layers import Conv2DTranspose, Conv2D\n","from tensorflow.python.keras.optimizers import Adam\n","from tensorflow.python.keras.models import Model\n","import numpy as np\n","\n","# 定义生成器模型\n","def build_generator(z_dim, num_classes, img_shape):\n"," noise_input = Input(shape=(z_dim, ))\n"," label_input = Input(shape=(1, ), dtype='int32')\n"," label_embedding = Embedding(num_classes, z_dim)(label_input)\n"," label_embedding = Flatten()(label_embedding)\n"," joined_representation = multiply([noise_input, label_embedding])\n"," generator = Dense(256, input_dim=z_dim*num_classes)(joined_representation)\n"," generator = LeakyReLU(alpha=0.2)(generator)\n"," generator = BatchNormalization(momentum=0.8)(generator)\n"," generator = Dense(512)(generator)\n"," generator = LeakyReLU(alpha=0.2)(generator)\n"," generator = BatchNormalization(momentum=0.8)(generator)\n"," generator = Dense(1024)(generator)\n"," generator = LeakyReLU(alpha=0.2)(generator)\n"," generator = BatchNormalization(momentum=0.8)(generator)\n"," generator = Dense(np.prod(img_shape), activation='tanh')(generator)\n"," generator = Reshape(img_shape)(generator)\n"," gen_model = Model(inputs=[noise_input, label_input], outputs=[generator])\n"," return gen_model\n","\n","# 定义判别器模型\n","def build_discriminator(img_shape, num_classes):\n"," img_input = Input(shape=img_shape)\n"," label_input = Input(shape=(1, ), dtype='int32')\n"," label_embedding = Embedding(num_classes, np.prod(img_shape))(label_input)\n"," label_embedding = Flatten()(label_embedding)\n"," flat_img = Flatten()(img_input)\n"," merged_input = Concatenate([flat_img, label_embedding])\n"," discriminator = Dense(1024)(merged_input)\n"," discriminator = LeakyReLU(alpha=0.2)(discriminator)\n"," discriminator = Dense(512)(discriminator)\n"," discriminator = LeakyReLU(alpha=0.2)(discriminator)\n"," discriminator = Dense(256)(discriminator)\n"," discriminator = LeakyReLU(alpha=0.2)(discriminator)\n"," discriminator = Dense(1, activation='sigmoid')(discriminator)\n"," disc_model = Model(inputs=[img_input, label_input], outputs=[discriminator])\n"," return disc_model\n","\n","# 定义CGAN模型\n","def build_cgan(generator, discriminator):\n"," z_dim = generator.input_shape[0][1]\n"," num_classes = discriminator.input_shape[1][1]\n"," noise_input = generator.input[0]\n"," label_input = generator.input[1]\n"," img = generator([noise_input, label_input])\n"," discriminator.trainable = False\n"," valid = discriminator([img, label_input])\n"," cgan = Model([noise_input, label_input], valid)\n"," cgan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n"," return cgan\n","\n","# 定义训练函数\n","def train_cgan(generator, discriminator, cgan, X_train, y_train, z_dim, num_classes,\n","epochs, batch_size):\n"," valid = np.ones((batch_size, 1))\n"," fake = np.zeros((batch_size, 1))\n"," for epoch in range(epochs):\n","  for _ in range(X_train.shape[0] // batch_size):\n","    idx = np.random.randint(0, X_train.shape[0], batch_size)\n","    real_images = X_train[idx]\n","    labels = y_train[idx]\n","    noise = np.random.normal(0, 1, (batch_size, z_dim))\n","    gen_images = generator.predict([noise, labels])\n","    d_loss_real = discriminator.train_on_batch([real_images, labels], valid)\n","    d_loss_fake = discriminator.train_on_batch([gen_images, labels], fake)\n","    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","    noise = np.random.normal(0, 1, (batch_size, z_dim))\n","    valid_labels = np.ones((batch_size, 1))\n","    g_loss = cgan.train_on_batch([noise, labels], valid_labels)\n","  print(f\"Epoch {epoch}, D Loss : {d_loss[0]}, G Loss : {g_loss}\")\n"," return generator\n","\n","# 设置参数\n","z_dim = 100 # 噪声向量维度\n","num_classes = N # 类别数量\n","img_shape = (64, 64, 3) # 图像形状\n","epochs = 10000\n","batch_size = 64\n","# 创建并编译生成器和判别器\n","generator = build_generator(z_dim, num_classes, img_shape)\n","discriminator = build_discriminator(img_shape, num_classes)\n","discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5),\n","metrics=['accuracy'])\n","# 创建并编译CGAN模型\n","cgan = build_cgan(generator, discriminator)\n","# 加载和准备数据（请根据实际情况更改）\n","X_train = ...\n","y_train = ...\n","# 训练CGAN模型\n","trained_generator = train_cgan(generator, discriminator, cgan, X_train, y_train,\n","              z_dim, num_classes, epochs, batch_size)\n","# 生成表情图像的示例\n","noise = np.random.normal(0, 1, (1, z_dim))\n","label = np.array([0]) # 替换为所需的标签\n","generated_image = trained_generator.predict([noise, label])"],"metadata":{"id":"E92GgHnvcVj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于编码器-解码器的表情生成\n","#VAE Python 代码示例如下。\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Lambda\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","import numpy as np\n","# 定义VAE模型\n","def build_vae(input_dim, latent_dim):\n"," # 编码器\n"," input_img = Input(shape=(input_dim, ))\n"," encoder = Dense(256, activation='relu')(input_img)\n"," z_mean = Dense(latent_dim)(encoder)\n"," z_log_var = Dense(latent_dim)(encoder)\n","\n"," # 采样层\n"," def sampling(args):\n"," z_mean, z_log_var = args\n"," epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n"," return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","\n"," z = Lambda(sampling)([z_mean, z_log_var])\n","\n"," # 解码器\n"," decoder_input = Input(shape=(latent_dim, ))\n"," decoder = Dense(256, activation='relu')(decoder_input)\n"," output_img = Dense(input_dim, activation='sigmoid')(decoder)\n","\n"," # 构建编码器和解码器\n"," encoder_model = Model(input_img, [z_mean, z_log_var, z])\n"," decoder_model = Model(decoder_input, output_img)\n","\n"," # 构建VAE模型\n"," output_img = decoder_model(encoder_model(input_img)[2])\n"," vae = Model(input_img, output_img)\n","\n"," # 定义VAE的损失函数\n"," reconstruction_loss = tf.keras.losses.binary_crossentropy(input_img, output_img)\n"," reconstruction_loss *= input_dim\n"," kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n"," kl_loss = K.sum(kl_loss, axis=-1)\n"," kl_loss *= -0.5\n"," vae_loss = K.mean(reconstruction_loss + kl_loss)\n"," vae.add_loss(vae_loss)\n","\n"," return vae, encoder_model, decoder_model\n","\n","# 设置参数\n","input_dim = 64 * 64 * 3 # 图像数据维度\n","latent_dim = 100 # 潜在空间维度\n","# 创建并编译VAE模型\n","vae, encoder, decoder = build_vae(input_dim, latent_dim)\n","vae.compile(optimizer='adam')\n","# 加载和准备数据（请根据实际情况更改）\n","X_train = ...\n","# 训练VAE模型\n","vae.fit(X_train, epochs=epochs, batch_size=batch_size)\n","# 使用VAE生成表情图像的示例\n","z_sample = np.random.normal(0, 1, (1, latent_dim))\n","generated_image = decoder.predict(z_sample)"],"metadata":{"id":"rkKg_Ae3drFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于迁移学习的表情生成（修改）\n","#以下是一个简化的Python代码示例，使用图像生成模型laion/DALLE2-PyTorch来生成数字人表情。\n","!pip install dalle2-pytorch\n","\n","import torch\n","from torchvision.transforms import ToPILImage\n","from dalle2_pytorch import DiffusionPrior, DiffusionPriorNetwork, OpenAIClipAdapter, Decoder, DALLE2\n","from dalle2_pytorch.train_configs import TrainDiffusionPriorConfig, TrainDecoderConfig\n","\n","prior_config = TrainDiffusionPriorConfig.from_json_path(\"weights/prior_config.json\").prior\n","prior = prior_config.create().cuda()\n","\n","prior_model_state = torch.load(\"weights/prior_latest.pth\")\n","prior.load_state_dict(prior_model_state, strict=True)\n","\n","decoder_config = TrainDecoderConfig.from_json_path(\"weights/decoder_config.json\").decoder\n","decoder = decoder_config.create().cuda()\n","\n","decoder_model_state = torch.load(\"weights/decoder_latest.pth\")[\"model\"]\n","\n","for k in decoder.clip.state_dict().keys():\n","    decoder_model_state[\"clip.\" + k] = decoder.clip.state_dict()[k]\n","\n","decoder.load_state_dict(decoder_model_state, strict=True)\n","\n","dalle2 = DALLE2(prior=prior, decoder=decoder).cuda()\n","\n","images = dalle2(\n","    ['一个带笑脸的数字人'],\n","    cond_scale = 2.\n",").cpu()\n","\n","print(images.shape)\n","\n","for img in images:\n","    img = ToPILImage()(img)\n","    img.show()"],"metadata":{"id":"mgJ7s6x5eJ6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.2.3　表情跟踪\n","#1．基于视频序列的表情跟踪\n","import cv2\n","# 读取视频文件\n","cap = cv2.VideoCapture('face_expression_video.mp4')\n","# 创建人脸检测器\n","face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","# 创建光流法对象\n","lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS + cv2.\n","TERM_CRITERIA_COUNT, 10, 0.03))\n","# 初始化特征点\n","old_frame = None\n","while cap.isOpened():\n"," ret, frame = cap.read()\n"," if not ret:\n","  break\n"," gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n"," faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n"," for (x, y, w, h) in faces:\n","  roi_gray = gray[y:y + h, x:x + w]\n","  p0 = cv2.goodFeaturesToTrack(roi_gray, maxCorners=100, qualityLevel=0.3, minDistance=7)\n","  # 计算光流\n","  p1, st, err = cv2.calcOpticalFlowPyrLK(roi_gray, gray, p0, None, **lk_params)\n","  # 在图像上绘制光流轨迹\n","  for i, (new, old) in enumerate(zip(p1, p0)):\n","    if st[i] == 1:\n","      a, b = new.ravel()\n","      c, d = old.ravel()\n","      frame = cv2.circle(frame, (a, b), 5, (0, 0, 255), -1)\n"," cv2.imshow(\"Face Expression Tracking\", frame)\n"," k = cv2.waitKey(30) & 0xff\n"," if k == 27:\n","  break\n","cap.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"BM879vedLlNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于特征点的表情跟踪(修改)\n","#下面是一个简化的Python代码示例，展示了使用Menpo库实现AAM表情跟踪的方法。\n","from menpofit.aam import PatchAAM\n","from menpofit.aam import LucasKanadeAAMFitter, WibergInverseCompositional\n","from menpodetect import load_dlib_frontal_face_detector\n","import menpo.io as mio\n","import matplotlib.pyplot as plt\n","\n","#训练PatchAAM模型\n","patch_aam = PatchAAM(<training_images>, group='PTS', patch_shape=[(15, 15), (23, 23)],\n","                     diagonal=150, scales=(0.5, 1.0),\n","                     max_shape_components=20, max_appearance_components=150,\n","                     verbose=True)\n","\n","#Lucas-Kanade推理器\n","fitter = LucasKanadeAAMFitter(patch_aam, lk_algorithm_cls=WibergInverseCompositional,\n","                     n_shape=[5, 20], n_appearance=[30, 150])\n","print(fitter)\n","\n","#==如果不想自己训练模型，也可以使用预训练模型\n","from menpofit.aam.pretrained import load_balanced_frontal_face_fitter\n","\n","fitter = load_balanced_frontal_face_fitter()\n","#==使用预训练模型结束\n","\n","#加载人脸检测器\n","detect = load_dlib_frontal_face_detector()\n","\n","#加载原图像并转化成灰度图像\n","image = mio.import_image('<要跟踪的图像路径>')\n","image = image.as_greyscale()\n","\n","#脸部检测\n","bboxes = detect(image)\n","\n","#裁剪图像\n","image = image.crop_to_landmarks_proportion(0.3, group='dlib_0')\n","bboxes[0] = image.landmarks['dlib_0'].lms\n","\n","if len(bboxes) > 0:\n","    #推理\n","    result = fitter.fit_from_bb(image, bboxes[0], max_iters=[15, 5],\n","                    gt_shape=image.landmarks['PTS'].lms)\n","    print(result)\n","\n","    #结果展示\n","    plt.subplot(131);\n","    image.view()\n","    bboxes[0].view(line_width=3, render_markers=False)\n","    plt.gca().set_title('Bounding box')\n","\n","    plt.subplot(132)\n","    image.view()\n","    result.initial_shape.view(marker_size=4)\n","    plt.gca().set_title('Initial shape')\n","\n","    plt.subplot(133)\n","    image.view()\n","    result.final_shape.view(marker_size=4, figure_size=(15, 13))\n","    plt.gca().set_title('Final shape')\n","\n","#更多内容请参考Menpo-AAM.ipynb"],"metadata":{"id":"LWY5j_YjMvKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于运动模型的表情跟踪（修改）\n","#下面是CLM算法的Python代码示例。\n","\n","#使用OpenFace命令行对视频中的人脸做表情跟踪，OpenFace内置了面部标志检测器和跟踪模型被卷积专家、约束局部模型 (CE-CLM)模型\n","!./OpenFace/build/bin/FaceLandmarkVidMulti -f video.mp4 -out_dir processed\n","\n","#将视频转换成mp4格式\n","!ffmpeg -y -loglevel info -i processed/video.avi output.mp4\n","\n","#显示结果\n","def show_local_mp4_video(file_name, width=640, height=480):\n","  import io\n","  import base64\n","  from IPython.display import HTML\n","  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n","  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n","                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n","                      </video>'''.format(width, height, video_encoded.decode('ascii')))\n","\n","show_local_mp4_video('output.mp4', width=960, height=720)\n","\n","#获取相关数据\n","import pandas as pd, seaborn as sns\n","sns.set_style('white')\n","import matplotlib.pyplot as plt\n","\n","df = pd.read_csv('processed/video.csv')\n","print(f\"Max number of frames {df.frame.max()}\", f\"\\nTotal shape of dataframe {df.shape}\")\n","df.head()\n","\n","#视频中有几张脸\n","print(\"Number of unique faces: \", len(df.face_id.unique()), \"\\nList of face_id's: \", df.face_id.unique())\n","\n","#更多数据分析请参看OpenFace_Shared.ipynb"],"metadata":{"id":"9Fx_vzTsN9Rz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.2.4　表情融合\n","#1．基于混合模型的表情融合（修改）\n","#以下是一个基本的Python代码示例，演示如何用3DMM（BFM）模型将马斯克的脸与爱因斯坦的脸相融合。\n","import dlib\n","import cv2\n","import numpy as np\n","from scipy.io import loadmat\n","import torch\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# 加载3DMM模型（示例使用BFM模型）\n","def load_3dmm_model(model_path):\n","    data = loadmat(model_path)\n","    return data['shapeMU'], data['shapePC'], data['shapeEV'], data['texMU'], data['texPC'], data['texEV']\n","\n","# 预处理图像，进行人脸对齐\n","def preprocess_image(image_path, detector, predictor):\n","    img = cv2.imread(image_path)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = detector(gray)\n","    if len(faces) == 0:\n","        raise ValueError(\"No face detected\")\n","    landmarks = predictor(gray, faces[0])\n","    return img, landmarks\n","\n","# 计算人脸的3D形状和纹理（假设使用3DMM的基本形式）\n","def compute_3d_face(shapeMU, shapePC, shapeEV, texMU, texPC, texEV, beta, gamma):\n","    shape = shapeMU + np.dot(shapePC, beta * shapeEV)\n","    texture = texMU + np.dot(texPC, gamma * texEV)\n","    return shape, texture\n","\n","# 融合两个3D人脸\n","def blend_faces(shape1, shape2, texture1, texture2, alpha=0.5):\n","    shape_blend = (1 - alpha) * shape1 + alpha * shape2\n","    texture_blend = (1 - alpha) * texture1 + alpha * texture2\n","    return shape_blend, texture_blend\n","\n","# 渲染图像（简化版，实际应用中可以使用更复杂的渲染工具）\n","def render_face(shape, texture):\n","    # 这里可以用OpenGL或其他渲染工具来渲染3D人脸\n","    # 本示例仅生成一个简单的彩色图像\n","    height, width = 256, 256\n","    image = np.zeros((height, width, 3), dtype=np.uint8)\n","    return image\n","\n","# 主流程\n","def main():\n","    # 配置路径\n","    model_path = 'path_to_3dmm_model.mat'  # 3DMM模型路径\n","    elon_path = 'path_to_elon_image.jpg'    # 马斯克的脸\n","    einstein_path = 'path_to_einstein_image.jpg'  # 爱因斯坦的脸\n","\n","    # 加载3DMM模型\n","    shapeMU, shapePC, shapeEV, texMU, texPC, texEV = load_3dmm_model(model_path)\n","\n","    # 初始化dlib人脸检测器和标志点预测器\n","    detector = dlib.get_frontal_face_detector()\n","    predictor = dlib.shape_predictor(dlib.shape_predictor_model_location())\n","\n","    # 预处理图像并计算3D人脸\n","    elon_img, elon_landmarks = preprocess_image(elon_path, detector, predictor)\n","    einstein_img, einstein_landmarks = preprocess_image(einstein_path, detector, predictor)\n","\n","    # 使用3DMM模型计算3D人脸形状和纹理\n","    elon_shape, elon_texture = compute_3d_face(shapeMU, shapePC, shapeEV, texMU, texPC, texEV, beta=np.random.randn(len(shapePC)), gamma=np.random.randn(len(texPC)))\n","    einstein_shape, einstein_texture = compute_3d_face(shapeMU, shapePC, shapeEV, texMU, texPC, texEV, beta=np.random.randn(len(shapePC)), gamma=np.random.randn(len(texPC)))\n","\n","    # 融合人脸\n","    blended_shape, blended_texture = blend_faces(elon_shape, einstein_shape, elon_texture, einstein_texture, alpha=0.5)\n","\n","    # 渲染图像\n","    blended_image = render_face(blended_shape, blended_texture)\n","\n","    # 显示图像\n","    plt.imshow(blended_image)\n","    plt.axis('off')\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"gI9ZmFv5aI-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于概率图模型的表情融合\n","#下面是一个简化的示例代码，演示了如何使用Python进行基于G-CRF的表情融合。\n","#!pip install -U cython\n","#!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n","import cv2\n","import numpy as np\n","import pydensecrf.densecrf as dcrf\n","import pydensecrf.utils as dcrf_utils\n","# 加载马斯克和爱因斯坦的图像\n","mask_image = cv2.imread(\"mask.jpg\") # 替换为马斯克脸的图像路径\n","einstein_image = cv2.imread(\"einstein.jpg\") # 替换为爱因斯坦脸的图像路径\n","# 打开摄像头\n","cap = cv2.VideoCapture(0)\n","while True :\n"," ret, frame = cap.read()\n"," # 确保图像大小一致\n"," mask_image = cv2.resize(mask_image, (frame.shape[1], frame.shape[0]))\n"," einstein_image = cv2.resize(einstein_image, (frame.shape[1], frame.shape[0]))\n"," # 创建一个掩码来指定融合区域\n"," mask = np.zeros_like(frame, dtype=np.uint8)\n"," mask[ :frame.shape[0] // 2, :, :] = 255 # 上半部分为马斯克，下半部分为爱因斯坦\n"," # 使用G-CRF算法进行图像融合\n"," blended_image = np.copy(frame)\n"," crf = dcrf.DenseCRF2D(frame.shape[1], frame.shape[0], 3)\n","\n"," U = -np.log(mask_image / 255.0 + 1e-3)\n"," U = U.transpose(2, 0, 1).reshape((3, -1))\n","\n"," crf.setUnaryEnergy(U)\n","\n"," d = dcrf_utils.createPairwiseBilateral(sdims=(10, 10), schan=(0.01, ), img=frame, chdim=2)\n"," crf.addPairwiseEnergy(d, compat=10)\n","\n"," d = dcrf_utils.createPairwiseGaussian(sxy=(1, 1), img=frame, chdim=2)\n"," crf.addPairwiseEnergy(d, compat=3)\n","\n"," Q = crf.inference(5)\n"," Q = np.argmax(np.array(Q), axis=0).reshape((frame.shape[0], frame.shape[1]))\n"," for c in range(3):\n","  blended_image[ :, :, c] = (1 - Q) * frame[ :, :, c] + Q * einstein_image[ :, :, c]\n"," # 显示结果图像\n"," cv2.imshow(\"G-CRF-based Facial Expression Fusion\", blended_image)\n"," # 退出循环\n"," if cv2.waitKey(1) & 0xFF == ord('q') : # 按q键退出\n","  break\n","# 释放摄像头和关闭窗口\n","cap.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"NLGz-PhfqAL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于风格迁移的表情融合（部分修改）\n","#下面是一个简化的代码示例，演示了如何使用PyTorch库进行神经风格迁移，将一个图像的风格应用到另一个图像上。\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models, transforms\n","from PIL import Image\n","\n","# 加载内容图像和风格图像\n","content_image = Image.open(\"content.jpg\")\n","style_image = Image.open(\"style.jpg\")\n","\n","# 转换图像大小并对其进行规范化\n","preprocess = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","content_tensor = preprocess(content_image).unsqueeze(0) # 添加批次维度\n","style_tensor = preprocess(style_image).unsqueeze(0)\n","\n","# 使用GPU（如果可用）\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 将图像移动到GPU\n","content_tensor = content_tensor.to(device)\n","style_tensor = style_tensor.to(device)\n","\n","# 加载预训练的VGG模型，用于特征提取\n","vgg = models.vgg19(pretrained=True).features.to(device).eval()\n","\n","# 定义损失函数，包括内容损失和风格损失\n","class ContentLoss(nn.Module):\n","    def __init__(self, target):\n","        super(ContentLoss, self).__init__()\n","        self.target = target.detach()\n","    def forward(self, x):\n","        loss = nn.functional.mse_loss(x, self.target)\n","        return loss\n","\n","class StyleLoss(nn.Module):\n","    def __init__(self, target):\n","        super(StyleLoss, self).__init__()\n","        self.target = self.gram_matrix(target).detach()\n","    def forward(self, x):\n","        G = self.gram_matrix(x)\n","        loss = nn.functional.mse_loss(G, self.target)\n","        return loss\n","    def gram_matrix(self, input):\n","        a, b, c, d = input.size()\n","        features = input.view(a * b, c * d)\n","        G = torch.mm(features, features.t())\n","        return G.div(a * b * c * d)\n","\n","# 定义内容损失和风格损失计算模块\n","content_criterion = ContentLoss(content_tensor)\n","style_criterion = StyleLoss(style_tensor)\n","\n","# 定义生成图像，初始为内容图像的副本\n","generated_image = content_tensor.clone().requires_grad_(True)\n","\n","# 定义优化器\n","optimizer = optim.LBFGS([generated_image])\n","\n","# 定义损失函数权重\n","content_weight = 1  # 调整权重以控制内容与风格之间的平衡\n","style_weight = 1000\n","\n","# 迭代优化过程\n","num_steps = 300\n","for step in range(num_steps):\n","    def closure():\n","        optimizer.zero_grad()\n","        # 获取模型的特征图\n","        content_features = vgg(content_tensor)\n","        style_features = vgg(style_tensor)\n","        generated_features = vgg(generated_image)\n","\n","        # 计算内容损失\n","        content_loss = content_weight * content_criterion(generated_features, content_features)\n","        # 计算风格损失\n","        style_loss = style_weight * style_criterion(generated_features, style_features)\n","        # 总损失\n","        total_loss = content_loss + style_loss\n","        total_loss.backward()\n","        return total_loss\n","\n","    optimizer.step(closure)\n","\n","# 将生成的图像从张量（矩阵格式）转换回图像格式，为显示图像做准备\n","output_image = generated_image.squeeze(0).cpu().clone()\n","output_image = output_image.clamp(0, 1)\n","output_image = transforms.ToPILImage()(output_image)\n","\n","# 显示融合后的图像\n","output_image.show()\n","#更多请参考https://github.com/gordicaleksa/pytorch-neural-style-transfer"],"metadata":{"id":"eTaNQu_ctADh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.3　姿态估计\n","#3.3.1　2D姿态估计\n","#1．基于热力图的方法（修改）\n","#以下是使用mmpose库加载HRNet模型并进行2D姿态估计的python代码示例。\n","#!pip install mmpose mmcv-full opencv-python\n","import cv2\n","import numpy as np\n","import torch\n","from mmpose.apis import (init_pose_model, inference_top_down_pose_model,\n","                         vis_pose_result)\n","from mmpose.datasets import DatasetInfo\n","from mmpose.datasets.pipelines import Compose\n","\n","# 配置文件路径\n","config_file = 'configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w32_coco_256x192.py'\n","# 预训练模型权重路径\n","checkpoint_file = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-6e6b7ec6_20200708.pth'\n","\n","# 初始化HRNet模型\n","model = init_pose_model(config_file, checkpoint_file, device='cuda:0')\n","\n","# 读取输入图像\n","image_path = 'input_image.jpg'\n","image = cv2.imread(image_path)\n","\n","# 生成检测结果\n","person_results = [{'bbox': [50, 50, 200, 200]}]  # 示例中假设已知人体边界框（bbox），实际应用中需要用检测器获得\n","\n","# 推理2D姿态\n","pose_results, _ = inference_top_down_pose_model(\n","    model,\n","    image,\n","    person_results,\n","    bbox_thr=0.3,\n","    format='xyxy',\n","    dataset='TopDownCocoDataset',\n","    dataset_info=None,\n","    return_heatmap=False,\n","    outputs=None\n",")\n","\n","# 可视化结果\n","vis_result = vis_pose_result(\n","    model,\n","    image,\n","    pose_results,\n","    dataset='TopDownCocoDataset',\n","    kpt_score_thr=0.3,\n","    show=False\n",")\n","\n","# 显示带有姿态估计结果的图像\n","cv2.imshow('Pose Estimation', vis_result)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"],"metadata":{"id":"MzE5CxUn7TL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于关键点回归的方法（修改）\n","#以下是使用SimpleBaseline进行2D姿态估计的代码示例。\n","import cv2\n","import torch\n","import numpy as np\n","import torchvision.transforms as transforms\n","from torchvision.models import resnet50\n","\n","# 定义SimpleBaseline模型类\n","class SimpleBaseline(torch.nn.Module):\n","    def __init__(self, backbone, num_keypoints=17):\n","        super(SimpleBaseline, self).__init__()\n","        self.backbone = backbone\n","        self.deconv_layers = self._make_deconv_layers()\n","        self.final_layer = torch.nn.Conv2d(\n","            in_channels=256,\n","            out_channels=num_keypoints,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0\n","        )\n","\n","    def _make_deconv_layers(self):\n","        layers = []\n","        for _ in range(3):\n","            layers.append(torch.nn.ConvTranspose2d(2048 if _ == 0 else 256, 256, kernel_size=4, stride=2, padding=1))\n","            layers.append(torch.nn.BatchNorm2d(256))\n","            layers.append(torch.nn.ReLU(inplace=True))\n","        return torch.nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = self.deconv_layers(x)\n","        x = self.final_layer(x)\n","        retur: x\n","\n","# 加载ResNet-50作为骨干网络\n","backbone = resnet50(pretrained=True)\n","# 去掉最后的全连接层\n","backbone = torch.nn.Sequential(*list(backbone.children())[:-2])\n","\n","# 初始化SimpleBaseline模型\n","model = SimpleBaseline(backbone)\n","# 加载预训练权重（需要本地文件路径）\n","model.load_state_dict(torch.load('simplebaseline_res50_coco.pth'))\n","model.eval()\n","\n","# 定义输入图像路径\n","image_path = 'input_image.jpg'\n","image = cv2.imread(image_path)\n","\n","# 预处理图像\n","input_size = (256, 192)\n","image_resized = cv2.resize(image, input_size)\n","image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n","\n","# 转换为Tensor并归一化\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","image_tensor = transform(image_rgb).unsqueeze(0)\n","\n","# 前向传播，获得关键点坐标序列\n","with torch.no_grad():\n","    heatmaps = model(image_tensor)\n","\n","# 后处理:将热图转换为关键点坐标\n","heatmaps = heatmaps.squeeze(0).cpu().numpy()\n","num_keypoints = heatmaps.shape[0]\n","\n","keypoint_coords = []\n","for i in range(num_keypoints):\n","    heatmap = heatmaps[i]\n","    y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n","    keypoint_coords.append((x, y))\n","\n","# 将关键点坐标映射回原始图像尺寸\n","keypoint_coords = np.array(keypoint_coords)\n","scale_x = image.shape[1] / input_size[1]\n","scale_y = image.shape[0] / input_size[0]\n","keypoint_coords[:, 0] *= scale_x\n","keypoint_coords[:, 1] *= scale_y\n","\n","# 输出姿态估计结果\n","print(\"Estimated keypoints:\", keypoint_coords)\n","\n","# 可视化关键点\n","for coord in keypoint_coords:\n","    cv2.circle(image, (int(coord[0]), int(coord[1])), 5, (0, 255, 0), -1)\n","\n","# 显示带有关键点的图像\n","cv2.imshow(\"Pose Estimation\", image)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()\n","#更多SimpleBaseline相关内容请参考https://github.com/microsoft/human-pose-estimation.pytorch"],"metadata":{"id":"0jUcj1jWBT0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于转换器的方法\n","#以下是使用TokenPose进行2D姿态估计的简化代码示例\n","import cv2\n","import torch\n","import numpy as np\n","import torchvision.transforms as transforms\n","from timm import create_model\n","\n","# 定义TokenPose模型类\n","class TokenPoseModel(torch.nn.Module):\n","    def __init__(self, model_name='tokenpose_s', num_keypoints=17):\n","        super(TokenPoseModel, self).__init__()\n","        self.model = create_model(model_name, pretrained=False, num_classes=num_keypoints*2)\n","\n","    def forward(self, x):\n","        output = self.model(x)\n","        output = output.reshape(-1, 17, 2)  # 假设输出每个关键点的 (x, y) 坐标\n","        return output\n","\n","# 初始化TokenPose模型\n","model = TokenPoseModel(model_name='tokenpose_s')\n","# 加载预训练权重（需要本地文件路径）\n","model.load_state_dict(torch.load('tokenpose_s_coco.pth'))\n","model.eval()\n","\n","# 读取输入图像\n","image_path = 'input_image.jpg'\n","image = cv2.imread(image_path)\n","orig_height, orig_width = image.shape[:2]\n","\n","# 预处理图像\n","input_size = (256, 192)  # TokenPose模型的输入大小\n","image_resized = cv2.resize(image, input_size)\n","image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n","\n","# 转换为Tensor并归一化\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","image_tensor = transform(image_rgb).unsqueeze(0)  # 添加批次维度\n","\n","# 前向传播，获得关键点坐标序列\n","with torch.no_grad():\n","    keypoints = model(image_tensor)\n","\n","# 将关键点映射回原始图像尺寸\n","keypoints = keypoints.squeeze(0).cpu().numpy()\n","keypoints[:, 0] *= (orig_width / input_size[1])\n","keypoints[:, 1] *= (orig_height / input_size[0])\n","\n","# 输出姿态估计结果\n","print(\"Estimated keypoints:\", keypoints)\n","\n","# 可视化关键点\n","for coord in keypoints:\n","    cv2.circle(image, (int(coord[0]), int(coord[1])), 5, (0, 255, 0), -1)\n","\n","# 显示带有关键点的图像\n","cv2.imshow(\"Pose Estimation\", image)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()\n","#更多TokenPose相关的训练和推理请参考https://github.com/leeyegy/TokenPose"],"metadata":{"id":"zLuxu_kfD3UE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.3.2　3D姿态估计\n","#1．基于单视角的方法（修改）\n","#以下是使用VIBE进行3D姿态估计的简化代码示例\n","import cv2\n","import time\n","import torch\n","import joblib\n","import shutil\n","import colorsys\n","import argparse\n","import numpy as np\n","from tqdm import tqdm\n","from multi_person_tracker import MPT\n","from torch.utils.data import DataLoader\n","\n","from lib.models.vibe import VIBE_Demo\n","from lib.utils.renderer import Renderer\n","from lib.dataset.inference import Inference\n","from lib.utils.smooth_pose import smooth_pose\n","from lib.data_utils.kp_utils import convert_kps\n","from lib.utils.pose_tracker import run_posetracker\n","\n","from lib.utils.demo_utils import (\n","    download_youtube_clip,\n","    smplify_runner,\n","    convert_crop_coords_to_orig_img,\n","    convert_crop_cam_to_orig_img,\n","    prepare_rendering_results,\n","    video_to_images,\n","    images_to_video,\n","    download_ckpt,\n",")\n","\n","MIN_NUM_FRAMES = 25\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","video_file = 'sample_video.mp4'\n","\n","image_folder, num_frames, img_shape = video_to_images(video_file, return_info=True)\n","\n","print(f'Input video number of frames {num_frames}')\n","orig_height, orig_width = img_shape[:2]\n","\n","#多人跟踪\n","mot = MPT(\n","    device=device,\n","    batch_size=args.tracker_batch_size,\n","    display=args.display,\n","    detector_type=args.detector,\n","    output_format='dict',\n","    yolo_img_size=args.yolo_img_size,\n",")\n","tracking_results = mot(image_folder)\n","\n","#定义VIBE模型\n","model = VIBE_Demo(\n","    seqlen=16,\n","    n_layers=2,\n","    hidden_size=1024,\n","    add_linear=True,\n","    use_residual=True,\n",").to(device)\n","\n","#加载预训练模型权重\n","pretrained_file = download_ckpt(use_3dpw=False)\n","ckpt = torch.load(pretrained_file)\n","print(f'Performance of pretrained model on 3DPW: {ckpt[\"performance\"]}')\n","ckpt = ckpt['gen_state_dict']\n","model.load_state_dict(ckpt, strict=False)\n","model.eval()\n","print(f'Loaded pretrained weights from \\\"{pretrained_file}\\\"')\n","\n","#对每个人分别使用VIBE模型推理\n","print(f'Running VIBE on each tracklet...')\n","vibe_time = time.time()\n","vibe_results = {}\n","for person_id in tqdm(list(tracking_results.keys())):\n","    bboxes = tracking_results[person_id]['bbox']\n","    joints2d = tracking_results[person_id]['joints2d']\n","\n","    frames = tracking_results[person_id]['frames']\n","\n","    dataset = Inference(\n","        image_folder=image_folder,\n","        frames=frames,\n","        bboxes=bboxes,\n","        joints2d=joints2d,\n","        scale=bbox_scale,\n","    )\n","\n","    bboxes = dataset.bboxes\n","    frames = dataset.frames\n","    has_keypoints = True if joints2d is not None else False\n","\n","    dataloader = DataLoader(dataset, batch_size=args.vibe_batch_size, num_workers=16)\n","\n","    with torch.no_grad():\n","\n","        pred_cam, pred_verts, pred_pose, pred_betas, pred_joints3d, smpl_joints2d, norm_joints2d = [], [], [], [], [], [], []\n","\n","        for batch in dataloader:\n","            if has_keypoints:\n","                batch, nj2d = batch\n","                norm_joints2d.append(nj2d.numpy().reshape(-1, 21, 3))\n","\n","            batch = batch.unsqueeze(0)\n","            batch = batch.to(device)\n","\n","            batch_size, seqlen = batch.shape[:2]\n","            output = model(batch)[-1]\n","\n","            pred_cam.append(output['theta'][:, :, :3].reshape(batch_size * seqlen, -1))\n","            pred_verts.append(output['verts'].reshape(batch_size * seqlen, -1, 3))\n","            pred_pose.append(output['theta'][:,:,3:75].reshape(batch_size * seqlen, -1))\n","            pred_betas.append(output['theta'][:, :,75:].reshape(batch_size * seqlen, -1))\n","            pred_joints3d.append(output['kp_3d'].reshape(batch_size * seqlen, -1, 3))\n","            smpl_joints2d.append(output['kp_2d'].reshape(batch_size * seqlen, -1, 2))\n","\n","\n","        pred_cam = torch.cat(pred_cam, dim=0)\n","        pred_verts = torch.cat(pred_verts, dim=0)\n","        pred_pose = torch.cat(pred_pose, dim=0)\n","        pred_betas = torch.cat(pred_betas, dim=0)\n","        pred_joints3d = torch.cat(pred_joints3d, dim=0)\n","        smpl_joints2d = torch.cat(smpl_joints2d, dim=0)\n","        del batch\n","\n","    #保存结果到.pkl文件\n","    pred_cam = pred_cam.cpu().numpy()\n","    pred_verts = pred_verts.cpu().numpy()\n","    pred_pose = pred_pose.cpu().numpy()\n","    pred_betas = pred_betas.cpu().numpy()\n","    pred_joints3d = pred_joints3d.cpu().numpy()\n","    smpl_joints2d = smpl_joints2d.cpu().numpy()\n","\n","    orig_cam = convert_crop_cam_to_orig_img(\n","        cam=pred_cam,\n","        bbox=bboxes,\n","        img_width=orig_width,\n","        img_height=orig_height\n","    )\n","\n","    joints2d_img_coord = convert_crop_coords_to_orig_img(\n","        bbox=bboxes,\n","        keypoints=smpl_joints2d,\n","        crop_size=224,\n","    )\n","\n","    output_dict = {\n","        'pred_cam': pred_cam,\n","        'orig_cam': orig_cam,\n","        'verts': pred_verts,\n","        'pose': pred_pose,\n","        'betas': pred_betas,\n","        'joints3d': pred_joints3d,\n","        'joints2d': joints2d,\n","        'joints2d_img_coord': joints2d_img_coord,\n","        'bboxes': bboxes,\n","        'frame_ids': frames,\n","    }\n","\n","    vibe_results[person_id] = output_dict\n","\n","del model\n","\n","end = time.time()\n","fps = num_frames / (end - vibe_time)\n","\n","print(f'VIBE FPS: {fps:.2f}')\n","total_time = time.time() - total_time\n","print(f'Total time spent: {total_time:.2f} seconds (including model loading time).')\n","print(f'Total FPS (including model loading time): {num_frames / total_time:.2f}.')\n","\n","print(f'Saving output results to \\\"{os.path.join(output_path, \"vibe_output.pkl\")}\\\".')\n","\n","joblib.dump(vibe_results, os.path.join(output_path, \"vibe_output.pkl\"))\n","\n","#渲染结果到一个视频文件\n","renderer = Renderer(resolution=(orig_width, orig_height), orig_img=True, wireframe=args.wireframe)\n","\n","output_img_folder = f'{image_folder}_output'\n","os.makedirs(output_img_folder, exist_ok=True)\n","\n","print(f'Rendering output video, writing frames to {output_img_folder}')\n","\n","frame_results = prepare_rendering_results(vibe_results, num_frames)\n","mesh_color = {k: colorsys.hsv_to_rgb(np.random.rand(), 0.5, 1.0) for k in vibe_results.keys()}\n","\n","image_file_names = sorted([\n","    os.path.join(image_folder, x)\n","    for x in os.listdir(image_folder)\n","    if x.endswith('.png') or x.endswith('.jpg')\n","])\n","\n","for frame_idx in tqdm(range(len(image_file_names))):\n","    img_fname = image_file_names[frame_idx]\n","    img = cv2.imread(img_fname)\n","\n","    for person_id, person_data in frame_results[frame_idx].items():\n","        frame_verts = person_data['verts']\n","        frame_cam = person_data['cam']\n","\n","        mc = mesh_color[person_id]\n","\n","        mesh_filename = None\n","\n","        img = renderer.render(\n","            img,\n","            frame_verts,\n","            cam=frame_cam,\n","            color=mc,\n","            mesh_filename=mesh_filename,\n","        )\n","\n","    cv2.imwrite(os.path.join(output_img_folder, f'{frame_idx:06d}.png'), img)\n","\n","    cv2.imshow('Video', img)\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cv2.destroyAllWindows()\n","\n","#保存最终的视频到sample_video_vibe_result.mp4\n","vid_name = os.path.basename(video_file)\n","save_name = f'{vid_name.replace(\".mp4\", \"\")}_vibe_result.mp4'\n","save_name = os.path.join(output_path, save_name)\n","print(f'Saving result video to {save_name}')\n","images_to_video(img_folder=output_img_folder, output_vid_file=save_name)\n","\n","#更多VIBE相关内容请参考https://github.com/mkocabas/VIBE"],"metadata":{"id":"kmUe5PW1YU2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于多视角的方法（修改）\n","#以下是一个简化的Multi-view Pose Transformer（MvP）多视角多人姿态端到端评估的代码示例。\n","import cv2\n","import datetime\n","import glob\n","import mmcv\n","import numpy as np\n","import os\n","from mmhuman3d.core.visualization.visualize_smpl import (\n","    visualize_smpl_calibration,\n",")\n","from mmhuman3d.utils.demo_utils import get_different_colors\n","from typing import List\n","from xrprimer.data_structure.camera import FisheyeCameraParameter\n","from xrprimer.utils.log_utils import setup_logger\n","\n","from xrmocap.core.estimation.builder import build_estimator\n","from xrmocap.visualization.visualize_keypoints3d import (\n","    visualize_keypoints3d_projected,\n",")\n","\n","#构建多视角多人姿态端到端评估器\n","estimator_config = dict(\n","    type='MultiViewMultiPersonEnd2EndEstimator',\n","    logger=logger,\n","    kps3d_model_path=args.model_dir)\n","estimator_config.update(dict(mmcv.Config.fromfile(args.estimator_config)))\n","smpl_estimator = build_estimator(estimator_config)\n","\n","#加载相机参数和图片\n","image_dir = []\n","fisheye_param_paths = []\n","with open(args.image_and_camera_param, 'r') as f:\n","    for i, line in enumerate(f.readlines()):\n","        line = line.strip()\n","        if i % 2 == 0:\n","            image_dir.append(line)\n","        else:\n","            fisheye_param_paths.append(line)\n","fisheye_params = load_camera_parameters(fisheye_param_paths)\n","mview_img_list = []\n","for idx in range(len(fisheye_params)):\n","    sview_img_list = sorted(\n","        glob.glob(os.path.join(image_dir[idx], '*.png')))\n","    img_list_start = int(sview_img_list[0][-10:-4])\n","    sview_img_list = sview_img_list[args.start_frame -\n","                      img_list_start:args.end_frame -\n","                      img_list_start]\n","\n","    mview_img_list.append(sview_img_list)\n","pred_keypoints3d, smpl_data_list = smpl_estimator.run(\n","    cam_param=fisheye_params, img_paths=mview_img_list)\n","npz_path = os.path.join(args.output_dir, 'pred_keypoints3d.npz')\n","pred_keypoints3d.dump(npz_path)\n","for i, smpl_data in enumerate(smpl_data_list):\n","    smpl_data.dump(os.path.join(args.output_dir, f'smpl_{i}.npz'))\n","\n","#可视化展示\n","\n","#准备保存路径\n","if not os.path.exists(os.path.join(args.output_dir, 'kps3d')):\n","    os.mkdir(os.path.join(args.output_dir, 'kps3d'))\n","n_frame = args.end_frame - args.start_frame\n","n_person = len(smpl_data_list)\n","colors = get_different_colors(n_person)\n","tmp = colors[:, 0].copy()\n","colors[:, 0] = colors[:, 2]\n","colors[:, 2] = tmp\n","full_pose_list = []\n","transl_list = []\n","betas_list = []\n","for smpl_data in smpl_data_list:\n","    full_pose_list.append(smpl_data['fullpose'][:, np.newaxis])\n","    transl_list.append(smpl_data['transl'][:, np.newaxis])\n","    betas_list.append(smpl_data['betas'][:, np.newaxis])\n","fullpose = np.concatenate(full_pose_list, axis=1)\n","transl = np.concatenate(transl_list, axis=1)\n","betas = np.concatenate(betas_list, axis=1)\n","\n","body_model_cfg = dict(\n","    type='SMPL',\n","    gender='neutral',\n","    num_betas=10,\n","    keypoint_src='smpl_45',\n","    keypoint_dst='smpl',\n","    model_path='xrmocap_data/body_models',\n","    batch_size=1)\n","\n","# prepare camera\n","for idx, fisheye_param in enumerate(fisheye_params):\n","    k_np = np.array(fisheye_param.get_intrinsic(3))\n","    r_np = np.array(fisheye_param.get_extrinsic_r())\n","    t_np = np.array(fisheye_param.get_extrinsic_t())\n","    cam_name = fisheye_param.name\n","    view_name = cam_name.replace('fisheye_param_', '')\n","\n","    image_list = []\n","    for frame_path in mview_img_list[idx]:\n","        image_np = cv2.imread(frame_path)\n","        image_list.append(image_np)\n","    image_array = np.array(image_list)\n","\n","    visualize_keypoints3d_projected(\n","        keypoints=pred_keypoints3d,\n","        camera=fisheye_param,\n","        output_path=os.path.join(args.output_dir, 'kps3d',\n","                      f'project_view_{view_name}.mp4'),\n","        background_arr=image_array.copy(),\n","        overwrite=True)\n","\n","    #展示SMPL模型校准\n","    visualize_smpl_calibration(\n","        poses=fullpose.reshape(n_frame, n_person, -1),\n","        betas=betas,\n","        transl=transl,\n","        palette=colors,\n","        output_path=os.path.join(args.output_dir, 'smpl',\n","                      f'{view_name}_smpl.mp4'),\n","        body_model_config=body_model_cfg,\n","        K=k_np,\n","        R=r_np,\n","        T=t_np,\n","        image_array=image_array,\n","        resolution=(image_array.shape[1], image_array.shape[2]),\n","        overwrite=True)\n","\n","#更多MvP算法的相关内容请参考https://github.com/openxrlab/xrmocap"],"metadata":{"id":"6E44D5E8icLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于参数化模型的方法(修改)\n","#以下是一个简化的代码示例，展示了使用SMPLify进行3D姿态估计的基本步骤。\n","import sys\n","import os\n","import os.path as osp\n","\n","import time\n","import yaml\n","import torch\n","\n","import smplx\n","\n","from utils import JointMapper\n","from cmd_parser import parse_config\n","from data_parser import create_dataset\n","from fit_single_frame import fit_single_frame\n","\n","from camera import create_camera\n","from prior import create_prior\n","\n","\n","dtype = torch.float32\n","\n","joint_mapper = JointMapper(dataset_obj.get_model2data())\n","\n","model_params = dict(model_path=args.get('model_folder'),\n","                    joint_mapper=joint_mapper,\n","                    create_global_orient=True,\n","                    create_body_pose=not args.get('use_vposer'),\n","                    create_betas=True,\n","                    create_left_hand_pose=True,\n","                    create_right_hand_pose=True,\n","                    create_expression=True,\n","                    create_jaw_pose=True,\n","                    create_leye_pose=True,\n","                    create_reye_pose=True,\n","                    create_transl=False,\n","                    dtype=dtype,\n","                    **args)\n","\n","male_model = smplx.create(gender='male', **model_params)\n","female_model = smplx.create(gender='female', **model_params)\n","\n","#创建相机对象\n","focal_length = args.get('focal_length')\n","camera = create_camera(focal_length_x=focal_length,\n","                    focal_length_y=focal_length,\n","                    dtype=dtype,\n","                    **args)\n","\n","if hasattr(camera, 'rotation'):\n","    camera.rotation.requires_grad = False\n","\n","use_hands = args.get('use_hands', True)\n","use_face = args.get('use_face', True)\n","\n","body_pose_prior = create_prior(\n","    prior_type=args.get('body_prior_type'),\n","    dtype=dtype,\n","    **args)\n","\n","jaw_prior, expr_prior = None, None\n","if use_face:\n","    jaw_prior = create_prior(\n","        prior_type=args.get('jaw_prior_type'),\n","        dtype=dtype,\n","        **args)\n","    expr_prior = create_prior(\n","        prior_type=args.get('expr_prior_type', 'l2'),\n","        dtype=dtype, **args)\n","\n","left_hand_prior, right_hand_prior = None, None\n","if use_hands:\n","    lhand_args = args.copy()\n","    lhand_args['num_gaussians'] = args.get('num_pca_comps')\n","    left_hand_prior = create_prior(\n","        prior_type=args.get('left_hand_prior_type'),\n","        dtype=dtype,\n","        use_left_hand=True,\n","        **lhand_args)\n","\n","    rhand_args = args.copy()\n","    rhand_args['num_gaussians'] = args.get('num_pca_comps')\n","    right_hand_prior = create_prior(\n","        prior_type=args.get('right_hand_prior_type'),\n","        dtype=dtype,\n","        use_right_hand=True,\n","        **rhand_args)\n","\n","shape_prior = create_prior(\n","    prior_type=args.get('shape_prior_type', 'l2'),\n","    dtype=dtype, **args)\n","\n","angle_prior = create_prior(prior_type='angle', dtype=dtype)\n","\n","if use_cuda and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","\n","    camera = camera.to(device=device)\n","    female_model = female_model.to(device=device)\n","    male_model = male_model.to(device=device)\n","    if args.get('model_type') != 'smplh':\n","        neutral_model = neutral_model.to(device=device)\n","    body_pose_prior = body_pose_prior.to(device=device)\n","    angle_prior = angle_prior.to(device=device)\n","    shape_prior = shape_prior.to(device=device)\n","    if use_face:\n","        expr_prior = expr_prior.to(device=device)\n","        jaw_prior = jaw_prior.to(device=device)\n","    if use_hands:\n","        left_hand_prior = left_hand_prior.to(device=device)\n","        right_hand_prior = right_hand_prior.to(device=device)\n","else:\n","    device = torch.device('cpu')\n","\n","#每个关节的权重\n","joint_weights = dataset_obj.get_joint_weights().to(device=device, dtype=dtype)\n","\n","joint_weights.unsqueeze_(dim=0)\n","\n","for idx, data in enumerate(dataset_obj):\n","  try:\n","    img = data['img']\n","    fn = data['fn']\n","    keypoints = data['keypoints']\n","    print('Processing: {}'.format(data['img_path']))\n","\n","    curr_result_folder = osp.join(result_folder, fn)\n","    if not osp.exists(curr_result_folder):\n","        os.makedirs(curr_result_folder)\n","    curr_mesh_folder = osp.join(mesh_folder, fn)\n","    if not osp.exists(curr_mesh_folder):\n","        os.makedirs(curr_mesh_folder)\n","    for person_id in range(keypoints.shape[0]):\n","        if person_id >= max_persons and max_persons > 0:\n","            continue\n","\n","        curr_result_fn = osp.join(curr_result_folder, '{:03d}.pkl'.format(person_id))\n","        curr_mesh_fn = osp.join(curr_mesh_folder, '{:03d}.obj'.format(person_id))\n","\n","        curr_img_folder = osp.join(output_folder, 'images', fn, '{:03d}'.format(person_id))\n","        if not osp.exists(curr_img_folder):\n","            os.makedirs(curr_img_folder)\n","\n","        if gender_lbl_type != 'none':\n","            if gender_lbl_type == 'pd' and 'gender_pd' in data:\n","                gender = data['gender_pd'][person_id]\n","            if gender_lbl_type == 'gt' and 'gender_gt' in data:\n","                gender = data['gender_gt'][person_id]\n","        else:\n","            gender = input_gender\n","\n","        if gender == 'neutral':\n","            body_model = neutral_model\n","        elif gender == 'female':\n","            body_model = female_model\n","        elif gender == 'male':\n","            body_model = male_model\n","\n","        out_img_fn = osp.join(curr_img_folder, 'output.png')\n","\n","        fit_single_frame(img, keypoints[[person_id]],\n","                  body_model=body_model,\n","                  camera=camera,\n","                  joint_weights=joint_weights,\n","                  dtype=dtype,\n","                  output_folder=output_folder,\n","                  result_folder=curr_result_folder,\n","                  out_img_fn=out_img_fn,\n","                  result_fn=curr_result_fn,\n","                  mesh_fn=curr_mesh_fn,\n","                  shape_prior=shape_prior,\n","                  expr_prior=expr_prior,\n","                  body_pose_prior=body_pose_prior,\n","                  left_hand_prior=left_hand_prior,\n","                  right_hand_prior=right_hand_prior,\n","                  jaw_prior=jaw_prior,\n","                  angle_prior=angle_prior,\n","                  **args)\n","  except:\n","      continue\n","\n","elapsed = time.time() - start\n","time_msg = time.strftime('%H hours, %M minutes, %S seconds',\n","                          time.gmtime(elapsed))\n","print('Processing the data took: {}'.format(time_msg))\n","\n","#更多SMPLify-X相关内容请参考https://github.com/KyujinHan/Smplify-X-Perfect-Implementation"],"metadata":{"id":"ikBdDKjFyQqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.3.3　手势估计与生成\n","#1．基于图像的方法\n","#以下是一个简化的代码示例，演示了如何使用Mesh MANO算法进行基于图像的手势估计。\n","import torch\n","from manopth.manolayer import ManoLayer\n","from manopth import demo\n","\n","batch_size = 10\n","\n","# 设置姿态空间的主成分数量\n","ncomps = 6\n","\n","# 初始化MANO层，用于生成手部网格\n","mano_layer = ManoLayer(mano_root='mano/models', use_pca=True, ncomps=ncomps)\n","\n","# 生成随机形状参数\n","# 这里的形状参数用于控制手部的形状变化\n","random_shape = torch.rand(batch_size, 10)\n","\n","# 生成随机姿态参数，包括全局旋转的轴角表示\n","# 姿态参数用于控制手部的姿态变化\n","random_pose = torch.rand(batch_size, ncomps + 3)\n","\n","# 通过MANO层进行前向传播，生成手部顶点和关节点\n","# 这里的形状和姿态参数被用来生成手部的3D网格\n","hand_verts, hand_joints = mano_layer(random_pose, random_shape)\n","demo.display_hand({'verts' : hand_verts, 'joints' : hand_joints}, mano_faces=mano_layer.th_faces)\n","\n","#更多Mesh MANO相关内容请参考https://github.com/hassony2/manopth"],"metadata":{"id":"jXy7mZ-sznWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于视频的方法\n","#以下是一个简化的代码示例，展示了如何使用Real-time-GesRec算法进行基于视频的手势估计。\n","import cv2\n","import torch\n","import torchvision.transforms as transforms\n","import numpy as np\n","from temporal_transforms import TemporalCenterCrop\n","from target_transforms import ClassLabel\n","\n","# 创建手势检测模型 (轻量级CNN)\n","gesture_detection_model = TemporalCenterCrop()\n","\n","# 创建手势分类模型 (深度CNN)\n","gesture_classification_model = ClassLabel()\n","\n","# 读取视频\n","cap = cv2.VideoCapture('your_video.mp4') # 替换为实际的视频文件路径\n","while cap.isOpened() :\n"," ret, frame = cap.read()\n","\n"," if not ret :\n"," break\n","\n"," # 预处理图像\n"," transform = transforms.Compose([\n","  transforms.ToPILImage(),\n","  transforms.Resize((224, 224)),\n","  transforms.ToTensor(),\n","  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n"," ])\n"," frame = transform(frame)\n","\n"," # 手势检测\n"," detection_result = gesture_detection_model(frame)\n","\n","  # 如果检测到手势\n"," if detection_result :\n","  # 手势分类\n","  gesture_class = gesture_classification_model(frame)\n","\n","  # 在图像上绘制检测结果和分类结果\n","  cv2.putText(frame, f'Gesture Class : {gesture_class}', (10, 30),\n","              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n","\n"," # 显示处理后的图像\n"," cv2.imshow('Real-time Gesture Recognition', frame)\n","\n"," if cv2.waitKey1） & 0xFF == ord('q') :\n","  break\n","cap.release()\n","cv2.destroyAllWindows()\n","\n","#更多相关内容请参考https://github.com/ahmetgunduz/Real-time-GesRec"],"metadata":{"id":"LLjspSf5Y8lf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．手势生成\n","#以下是一个简化的代码示例，演示了如何使用HOGAN进行手势生成。\n","import torch\n","from torch.utils.data import DataLoader\n","from hogan_model import HOGANModel # 替换为实际的HOGAN模型代码\n","from hogan_dataset import HOGANDataset # 替换为实际的数据集处理代码\n","from hogan_loss import HOGANLoss # 替换为实际的损失函数代码\n","from torch.optim import Adam\n","from visdom import Visdom\n","\n","# 数据准备\n","def prepare_data() :\n"," # 在这里加载手和物体互动的数据集，例如HO3Dv3和DexYCB\n"," dataset = HOGANDataset(...) # 替换为实际的数据集加载代码\n"," dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"," return dataloader\n","\n","# 模型训练\n","def train_hogan_model(model, dataloader, num_epochs=10, learning_rate=0.001) :\n"," criterion = HOGANLoss() # 替换为实际的损失函数\n"," optimizer = Adam(model.parameters(), lr=learning_rate)\n","\n"," for epoch in range(num_epochs) :\n","  for batch in dataloader :\n","    inputs, targets = batch\n","    outputs = model(inputs)\n","    loss = criterion(outputs, targets)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n"," print(f'Epoch {epoch+1}/{num_epochs}, Loss : {loss.item()}')\n","\n","# 结果展示\n","def display_results() :\n"," # 在这里运行visdom服务器并查看训练结果和损失情况\n"," viz = Visdom()\n"," # 在这里添加可视化代码，如用来绘制损失曲线、展示生成图像等的代码\n","\n","# 模型测试\n","def test_hogan_model(model, dataloader) :\n"," # 使用bash命令运行测试脚本（eval_hov3.sh）\n"," # 在这里添加代码运行测试脚本，查看测试结果\n","\n","# 主程序\n","if __name__ == \"__main__\" :\n"," # 数据准备\n"," dataloader = prepare_data()\n","\n"," # 创建并训练HOGAN模型\n"," hogan_model = HOGANModel(...) # 替换为实际的HOGAN模型代码\n"," train_hogan_model(hogan_model, dataloader)\n","\n"," # 结果展示\n"," display_results()\n","\n"," # 模型测试\n"," test_hogan_model(hogan_model, dataloader)\n","\n","#更多相关内容请参考https://github.com/play-with-HOI-generation/HOIG"],"metadata":{"id":"liq0BHe2vj8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#====================================================================#"],"metadata":{"id":"rPMkAz2K5t9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.4　口型匹配\n","#3.4.1　2D 唇型检测\n","#1．基于颜色空间的方法\n","#以下是一个基于HSV颜色模型的简单代码示例，用于实现2D唇型检测。\n","import cv2\n","import numpy as np\n","\n","def detect_lips_hsv(image) :\n"," # 将图像转换为HSV颜色空间\n"," hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n"," # 定义唇部颜色范围\n"," lower_bound = np.array([H_min, S_min, V_min])\n"," upper_bound = np.array([H_max, S_max, V_max])\n","\n"," # 根据颜色范围进行掩码操作\n"," mask = cv2.inRange(hsv_image, lower_bound, upper_bound)\n","\n"," # 执行形态学操作来增强唇部区域\n"," kernel = np.ones((5, 5), np.uint8)\n"," mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n"," mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n","\n"," # 在原始图像上应用掩码\n"," result = cv2.bitwise_and(image, image, mask=mask)\n"," return result\n","\n","# 调用函数进行唇型检测\n","image = cv2.imread('lip_image.jpg')\n","result_image = detect_lips_hsv(image)\n","\n","# 显示结果图像\n","cv2.imshow('Lip Detection', result_image)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"],"metadata":{"id":"S6S6qpWGzxnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于活动轮廓模型的方法\n","#以下是一个简化的ASM算法代码示例，用于实现2D唇型检测。请注意，实际的ASM算法需要大量的训练数据和模型训练时间，这里只提供了一个简单的示例以演示基本思想。\n","import cv2\n","import numpy as np\n","from skimage.io import imread\n","from skimage.color import rgb2gray\n","from skimage.transform import warp\n","from skimage.feature import canny\n","from skimage.measure import label, regionprops\n","from skimage.morphology import closing\n","from skimage.draw import polygon_perimeter\n","from skimage.morphology import dilation\n","\n","# 加载训练好的ASM模型\n","# 注意：这里需要有一个训练好的ASM模型文件，例如lip_asm_model.pkl\n","asm_model = load_asm_model('lip_asm_model.pkl')\n","\n","# 加载待检测的图像\n","image = imread('lip_image.jpg')\n","\n","# 将图像转换为灰度\n","gray_image = rgb2gray(image)\n","\n","# 应用Canny边缘检测\n","edges = canny(gray_image, sigma=2, low_threshold=10, high_threshold=30)\n","\n","# 应用形态学操作来去除噪声\n","kernel = closing((3, 3))\n","edges = kernel(edges)\n","\n","# 使用ASM模型进行唇部检测\n","lip_shape = asm_model.predict(gray_image)\n","\n","# 将唇部形状转换为轮廓\n","lip_contour = polygon_perimeter(lip_shape)\n","\n","# 在原始图像上绘制唇部轮廓\n","for contour in lip_contour:\n"," cv2.polylines(image, [contour], isClosed=True, color=(0, 255, 0), thickness=2)\n","\n","# 显示结果图像\n","cv2.imshow('Lip Detection', image)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"],"metadata":{"id":"gtNykPMy50xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于关键点检测的方法\n","#以下是一个简化的Hourglass Network算法代码示例，用于实现2D唇型检测。请注意，实际的Hourglass Network需要更大规模的数据和训练，这里只提供了一个基本的框架。\n","import torch\n","import torch.nn as nn\n","\n","# 定义Hourglass模块\n","class Hourglass(nn.Module) :\n"," def __init__(self, num_blocks, num_features) :\n","  super(Hourglass, self).__init__()\n","  # 构建Hourglass模块的卷积和上采样层\n","  # ...（这里应该添加具体的卷积和上采样层的构建代码）\n","\n"," def forward(self, x) :\n","  # Hourglass模块的前向传播逻辑\n","  # ...（这里应该添加具体的前向传播逻辑代码）\n","\n","# 定义Hourglass Network\n","class HourglassNet(nn.Module) :\n"," def __init__(self, num_stacks, num_blocks, num_features) :\n","  super(HourglassNet, self).__init__()\n","  # 构建Hourglass Network的多个Hourglass模块\n","  self.hourglass_modules = nn.ModuleList([Hourglass(num_blocks, num_features)\n","              for _ in range(num_stacks)])\n","  # ...（这里可以添加其他必要的网络层）\n","\n"," def forward(self, x) :\n","  # Hourglass Network的前向传播逻辑\n","  # ...（这里应该添加具体的前向传播逻辑代码）\n","\n","# 创建Hourglass Network模型\n","model = HourglassNet(num_stacks=2, num_blocks=4, num_features=256)\n","\n","# 加载待检测的图像并进行预处理\n","input_image = preprocess_image('lip_image.jpg')\n","\n","# 使用模型进行唇型关键点检测\n","with torch.no_grad() :\n"," keypoints = model(input_image)\n","\n","# 可视化检测结果\n","visualize_keypoints(input_image, keypoints)"],"metadata":{"id":"vm3aatna6Uwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.4.2　2D口型匹配\n","#1．基于GAN的方法\n","#以下是一个简化的Wav2Lip算法代码示例，用于将音频与静态人脸图像匹配，生成同步的嘴部运动\n","import wav2lip\n","\n","# 加载音频和人脸图像\n","audio = wav2lip.load_audio('audio.wav')\n","face_image = wav2lip.load_face_image('face.jpg')\n","\n","# 提取音频特征\n","audio_features = wav2lip.extract_audio_features(audio)\n","\n","# 检测嘴部关键点\n","mouth_keypoints = wav2lip.detect_mouth_keypoints(face_image)\n","\n","# 嘴部形状变换\n","transformed_mouth_shape = wav2lip.transform_mouth_shape(audio_features, mouth_keypoints)\n","\n","# 生成嘴部图像\n","mouth_image = wav2lip.generate_mouth_image(transformed_mouth_shape)\n","\n","# 合成视频\n","output_video = wav2lip.compose_video(face_image, mouth_image, audio)"],"metadata":{"id":"Jtr6e5pP8qix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于表情迁移的方法\n","#以下是一个简化的基于DeepFake的表情迁移代码示例，展示了如何使用DeepFake技术来实现口型匹配。\n","import deepfake\n","\n","# 加载源人物和目标人物的图像和视频数据\n","source_face = deepfake.load_image(\"source_face.jpg\")\n","target_face = deepfake.load_image(\"target_face.jpg\")\n","source_video = deepfake.load_video(\"source_video.mp4\")\n","\n","# 训练DeepFake模型\n","deepfake_model = deepfake.train(source_face, target_face, source_video)\n","\n","# 生成口型匹配的视频\n","output_video = deepfake.generate_video(source_video, deepfake_model)\n","\n","# 保存生成的视频\n","deepfake.save_video(output_video, \"output_video.mp4\")"],"metadata":{"id":"6j-28Jss9Bo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于LSTM的方法\n","#以下是一个简化的基于LSTM的口型匹配代码示例，展示了如何使用LSTM-based Lip Sync算法来实现口型匹配\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","\n","# 构建LSTM模型\n","model = Sequential()\n","model.add(LSTM(128, input_shape=(timesteps, input_dim)))\n","model.add(Dense(output_dim, activation='softmax'))\n","\n","# 编译和训练模型\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=10, batch_size=64)\n","\n","# 预测口型匹配\n","predictions = model.predict(X_test)"],"metadata":{"id":"PbJjatai9T3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.4.3　3D 唇型检测\n","#1．基于统计学模型的方法\n","#以下是一个简化的基于统计学模型的 3D 唇型检测代码示例，展示了如何使用 3DMM 来进行口型匹配。\n","import dlib\n","import numpy as np\n","from scipy.spatial import procrustes\n","\n","# 初始化人脸关键点检测器\n","predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n","\n","# 从图像中检测关键点\n","def detect_landmarks(image) :\n"," gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n"," rects = detector(gray)\n"," landmarks = []\n"," for rect in rects :\n"," shape = predictor(gray, rect)\n"," landmarks.append(shape)\n"," return landmarks\n","\n","# 计算嘴唇型状的3D重建\n","def reconstruct_lips_3d(landmarks, mean_shape_model, shape_model_components) :\n"," # 实现3D重建的代码\n"," # ...\n"," return reconstructed_lips\n","\n","# 嘴唇运动跟踪和口型匹配\n","def track_lip_movement(landmarks_sequence, mean_shape_model, shape_model_components) :\n"," # 实现运动估计和口型匹配的代码\n"," # ...\n"," return lip_movement\n","\n","# 示例代码的使用\n","image = cv2.imread('face_image.jpg')\n","landmarks = detect_landmarks(image)\n","reconstructed_lips = reconstruct_lips_3d(landmarks, mean_shape_model, shape_model_components)\n","lip_movement = track_lip_movement(landmarks_sequence, mean_shape_model, shape_model_components)"],"metadata":{"id":"7po3tiuT9rGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于 RGB-D 摄像的方法\n","#以下是一个简化的基于 RGB-D 摄像的 3D 唇型检测代码示例，展示了如何使用深度信息来进行口型匹配。\n","import cv2\n","import numpy as np\n","import open3d as o3d\n","\n","# 初始化深度摄像机\n","kinect = cv2.VideoCapture(cv2.CAP_OPENNI2)\n","if not kinect.isOpened() :\n"," raise Exception(\"Unable to open Kinect\")\n","\n","# 读取深度图像和RGB图像\n","ret, depth_frame = kinect.read()\n","ret, color_frame = kinect.read()\n","\n","# 嘴唇区域提取（示例）\n","lip_region = color_frame[100 :200, 200 :400]\n","\n","# 深度信息融合\n","depth_data = depth_frame[100 :200, 200 :400]\n","point_cloud = np.zeros((lip_region.shape[0], lip_region.shape[1], 3), dtype=np.float32)\n","for i in range(point_cloud.shape[0]) :\n"," for j in range(point_cloud.shape[1]) :\n","  depth = depth_data[i, j]\n","  if depth > 0 :\n","    point_cloud[i, j, 0] = j\n","    point_cloud[i, j, 1] = i\n","    point_cloud[i, j, 2] = depth\n","\n","# 创建点云对象\n","pcd = o3d.geometry.PointCloud()\n","pcd.points = o3d.utility.Vector3dVector(point_cloud)\n","\n","# 三维形状建模\n","o3d.visualization.draw_geometries([pcd])\n","\n","# 进行口型匹配预测\n","# ...\n","\n","# 关闭深度摄像机\n","kinect.release()"],"metadata":{"id":"JnBIjdmx-D8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于参数化人脸模型的方法\n","#以下是一个简化的基于参数化人脸模型的 3D 唇型检测代码示例，展示了如何使用 AAM 模型来进行口型匹配。\n","import dlib\n","import numpy as np\n","\n","# 初始化AAM模型\n","aam_model = dlib.shape_predictor('aam_model.dat')\n","\n","# 从图像中检测关键点\n","def detect_landmarks(image) :\n"," gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n"," shape = aam_model(gray)\n"," landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n"," return landmarks\n","\n","# 参数拟合\n","def fit_aam_model(landmarks, aam_model) :\n"," # 实现参数拟合的代码\n"," # ...\n"," return fitted_parameters\n","\n","# 嘴唇型状重建\n","def reconstruct_lips_shape(fitted_parameters) :\n"," # 实现形状重建的代码\n"," # ...\n"," return reconstructed_shape\n","\n","# 进行口型匹配预测\n","# ...\n","\n","# 示例代码的使用\n","image = cv2.imread('face_image.jpg')\n","landmarks = detect_landmarks(image)\n","fitted_parameters = fit_aam_model(landmarks, aam_model)\n","reconstructed_shape = reconstruct_lips_shape(fitted_parameters)"],"metadata":{"id":"FlAK-Ae5-0CV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.4.4　3D 口型匹配\n","#1．基于模型预测的方法\n","#以下是一个简化的伪代码示例，展示了如何使用 Audio2Face 算法生成 3D 口型动画。\n","#请注意，这只是一个概念示例，实际实现需要更多的细节和模型训练。\n","import deep_learning_library as dl\n","\n","# 加载预训练的Audio2Face模型\n","model = dl.load_audio2face_model()\n","\n","# 提取音频特征\n","audio_features = dl.extract_audio_features(audio_input)\n","\n","# 预测唇型参数\n","lip_parameters = model.predict(audio_features)\n","\n","# 生成3D口型\n","three_d_lip_model = dl.generate_3d_lip_model(lip_parameters)\n","\n","# 渲染和同步\n","rendered_video = dl.render_video(three_d_lip_model, audio_input)"],"metadata":{"id":"xlsWH4lb_R9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．基于深度学习的方法\n","#以下是一个简化的代码示例，展示了如何使用 Adobe 的 MakeItTalk 算法生成 3D 口型动画。\n","#请注意，这只是一个概念示例，实际实现需要更多的细节和深度学习框架支持。\n","import deep_learning_library as dl\n","\n","# 加载预训练的MakeItTalk模型\n","model = dl.load_makeittalk_model()\n","\n","# 提取音频特征或文本编码\n","audio_features = dl.extract_audio_features(audio_input)\n","text_encoding = dl.encode_text(text_input)\n","\n","# 预测唇部参数或特征\n","lip_features = model.predict(audio_features, text_encoding)\n","\n","# 生成3D口型\n","three_d_lip_model = dl.generate_3d_lip_model(lip_features)\n","\n","# 渲染和同步\n","rendered_video = dl.render_video(three_d_lip_model, audio_input)"],"metadata":{"id":"ra_DnOIY_m7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．基于神经渲染的方法\n","#RAD-NeRF 的实现示例代码如下。\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","# 定义主模型类\n","class RADNeRF(nn.Module):\n"," def __init__(self, audio_in_dim, audio_dim, in_dim, out_dim, hidden_dim, max_steps, grid_\n","size, density_bitfield, cascade):\n","  super(RADNeRF, self).__init__()\n","  self.audio_feature_extractor = AudioFeatureExtractor(audio_in_dim, audio_dim)\n","  self.ray_marcher = RayMarching(max_steps, grid_size, density_bitfield, cascade)\n","  self.nerf = NeRF(in_dim, out_dim, hidden_dim)\n","\n"," def forward(self, audio_features, rays_o, rays_d, nears, fars, ind_code, eye):\n","  # 提取音频特征\n","  audio_encoding = self.audio_feature_extractor(audio_features)\n","  # 光线行进\n","  xyzs, dirs, deltas = self.ray_marcher(rays_o, rays_d, nears, fars, audio_encoding)\n","  # 通过NeRF模型计算颜色、密度和环境光\n","  sigmas, rgbs, ambients = self.nerf(xyzs, dirs, audio_encoding, ind_code, eye)\n","  # 计算2D图像和深度\n","  image, depth = self.composite_rays(xyzs, dirs, sigmas, rgbs, deltas)\n","  return image, depth\n","\n","#ER-NeRF 的 Python 实现代码如下。\n","\n","# 获取空间区域特征\n","regional_feats = RegionalFeatureExtractor(space)\n","\n","# 计算注意力权重\n","attn_weights = Attention(audio_feat, regional_feats)\n","\n","# 与区域特征拼接\n"," regional_audio_feats = Concat([audio_feat, regional_feats, attn_weights])\n","\n","# NeRF\n","rgb, density = NeRF(regional_audio_feats)"],"metadata":{"id":"A6J937b0_2bO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#第4章 数字人语音合成\n","#4.1　语音数字化原理\n","#4.1.1 音频采样\n","#1．采样频率选择\n","#对语音合成任务而言，16kHz 采样率已经很好地平衡了语音质量与存储效率。我们可以用librosa 库来加载 16kHz 采样的语音。\n","import librosa\n","audio_path = 'english.wav'\n","Samples, sample_rate = librosa.load(audio_path， sr=16000)\n","\n","#也可以用 pydub 库实现音频重采样。\n","from pydub import AudioSegment\n","# 加载音频文件\n","audio = AudioSegment.from_file(\"input.wav\", format=\"wav\")\n","# 将采样率设置为16000Hz\n","audio = audio.set_frame_rate(16000)\n","# 导出重采样后的音频文件\n","audio.export(\"output.wav\", format=\"wav\")"],"metadata":{"id":"bYv1aFv0AdR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．量化位数选择\n","#使用Python的librosa库可以方便地读取16bit@16kHz语音。\n","import librosa\n","Samples, sample_rate = librosa.load(\"speech.wav\", sr=16000)"],"metadata":{"id":"izb3m3wHBRhj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.1.2　语音编码\n","#1．PCM 编码\n","#（3）µ法则与A法则\n","#在 Python 中 scipy 库提供了这两种算法的编码实现。\n","import librosa\n","from scipy.io import wavfile\n","\n","# μ法则编码\n","encoded = librosa.core.codec.mu_encode(samples, mu=255)\n","\n","# A法则编码\n","encoded = librosa.core.codec.a_encode(samples, a=87)"],"metadata":{"id":"SbqeTNq7Bfud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．LPC 编码\n","#（4）LPC 模型的 Python 实现\n","#下面使用 librosa 库和 scipy 库来实现 LPC 模型的代码。\n","#1）确保已经安装了 librosa 库和 scipy 库。这两个库将用于音频处理和 LPC 模型的实现。\n","pip install librosa scipy\n","\n","#2）导入所需的库。\n","import numpy as np\n","import librosa\n","from scipy.signal import lfilter\n","\n","#3）定义 lpc_analysis 函数。\n","def lpc_analysis(signal, order) :\n"," autocorr = np.correlate(signal, signal, mode='full')\n"," autocorr = autocorr[len(signal)-1 :]\n","\n"," r = np.array([-autocorr[i] for i in range(1, order+1)])\n"," R = np.array([[autocorr[i-j] for j in range(order)] for i in range(1, order+1)])\n"," a = np.dot(np.linalg.inv(R), r)\n","\n"," return a\n","\n","#4）定义 lpc_synthesis 函数。\n","def lpc_synthesis(a, excitation) :\n"," synthetic_signal = lfilter([1] + list(-a), [1], excitation)\n"," return synthetic_signal\n","\n","#5）读取音频文件。\n","filename = 'your_audio_file.wav'\n","signal, sr = librosa.load(filename, sr=None)\n","\n","#6）设置参数。\n","order = 10 # LPC阶数\n","frame_len = 240 # 每帧的样本数\n","\n","#7）分析并合成。\n","synthetic_signal = np.zeros_like(signal)\n","for i in range(0, len(signal), frame_len) :\n"," frame = signal[i :i+frame_len]\n"," if len(frame) < frame_len :\n","  break\n"," lpc_coeffs = lpc_analysis(frame, order)\n"," excitation = np.random.normal(0, 0.5, len(frame))\n"," synthetic_frame = lpc_synthesis(lpc_coeffs, excitation)\n"," synthetic_signal[i :i+frame_len] = synthetic_frame"],"metadata":{"id":"Pv6NvpQ2B5va"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．正弦编码\n","#（1）基频检测\n","import parselmouth\n","from parselmouth.praat import call\n","sound = parselmouth.Sound(\"speech.wav\")\n","pitch = call(sound, \"To Pitch\", 0.0, 75, 600)\n","# 提取基频曲线\n","pitch_values = pitch.selected_array['frequency']\n","\n","#（2）幅度谱建模\n","import librosa\n","import numpy as np\n","# 提取语音的幅度谱\n","amp_spect = np.abs(librosa.stft(speech))\n","# LPC预测全波段幅度谱\n","lpc_model = librosa.core.lpc(amp_spect, order=10)\n","\n","#（3）基频编码\n","from scipy.signal import quantize\n","# 量化基频参数\n","quant_pitch = quantize(pitch_values, 64, 'log')\n","# 基频参数的矢量量化\n","from sklearn.cluster import KMeans\n","kmeans = KMeans(n_clusters=32)\n","kmeans.fit(pitch_values[ ：, np.newaxis])\n","\n","#（4）幅度谱编码\n","import librosa\n","from sklearn.cluster import KMeans\n","# 幅度谱矢量量化\n","kmeans = KMeans(n_clusters=16)\n","kmeans.fit(amp_spect)\n","# 幅度谱LPC编码\n","lpc_coeffs = librosa.core.lpc(amp_spect, order=12)"],"metadata":{"id":"0FNJdLEsCzed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.2　基于拼接的语音合成\n","#4.2.1　段音拼接\n","#2．拼接方法\n","#（1）线性拼接\n","import librosa\n","# 加载两个音频片段\n","audio1, sr = librosa.load('audio1.wav')\n","audio2, sr = librosa.load('audio2.wav')\n","# 简单线性拼接\n","concat = np.concatenate((audio1, audio2))\n","# 保存拼接结果\n","librosa.output.write_wav('linear_concat.wav', concat, sr)\n","\n","#（2）叠加拼接\n","import numpy as np\n","# 加载音频片段\n","audio1, sr = librosa.load('audio1.wav')\n","audio2, sr = librosa.load('audio2.wav')\n","# 计算拼接处的重叠长度\n","overlap = int(sr * 0.01)\n","# 汉明窗加权叠加\n","window = np.hamming(overlap)\n","concat = np.concatenate((audio1[ :-overlap],\n"," audio1[-overlap :]*window + audio2[ :overlap]*window,\n"," audio2[overlap :]))\n","# 保存拼接语音\n","librosa.output.write_wav('overlap_concat.wav', concat, sr)\n","\n","#（3）多音素拼接\n","import librosa\n","import numpy as np\n","# 加载3段音频\n","audio1, sr = librosa.load('audio1.wav')\n","audio2, sr = librosa.load('audio2.wav')\n","audio3, sr = librosa.load('audio3.wav')\n","# 重叠区线性混合\n","concat = np.concatenate((audio1[ :-2],\n"," 0.5*audio1[-2 :] + 0.5*audio2[ :2],\n"," audio2[2 :-2],\n"," 0.5*audio2[-2 :] + 0.5*audio3[ :2],\n"," audio3[2 :]))\n","# 保存拼接语音\n","librosa.output.write_wav('multi_concat.wav', concat, sr)"],"metadata":{"id":"dAQiYROxC9ci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.2.2　语音跨段平滑\n","#1．最大似然连续化方法\n","#（2）实现代码\n","import numpy as np\n","import librosa\n","def smooth_transitions(audio, transition_prob) ：\n"," smoothed_audio = np.copy(audio)\n","\n"," # 进行状态转移平滑\n"," for i in range(1, len(audio)) ：\n"," smoothed_audio[i] = smoothed_audio[i-1] * transition_prob\n","\n"," return smoothed_audio\n","# 示例音频\n","filename = 'your_audio_file.wav'\n","audio, sr = librosa.load(filename, sr=None)\n","# 设置状态转移概率，示例中简化为一个常数\n","transition_prob = 0.95\n","# 进行状态转移平滑\n","smoothed_audio = smooth_transitions(audio, transition_prob)\n","# 保存合成的声音\n","output_filename = 'smoothed_audio.wav'\n","librosa.output.write_wav(output_filename, smoothed_audio, sr)"],"metadata":{"id":"W3r8Cgr6Eb-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2．隐马尔可夫模型\n","#（2）实现代码\n","import numpy as np\n","import librosa\n","from hmmlearn import hmm\n","def smooth_with_hmm(audio, n_states, transition_prob) ：\n"," model = hmm.GaussianHMM(n_components=n_states, covariance_type='diag')\n","\n"," # 训练HMM模型\n"," model.fit(audio.reshape(-1, 1))\n","\n"," # 预测状态序列\n"," _, states = model.decode(audio.reshape(-1, 1))\n","\n"," # 进行状态转移平滑\n"," smoothed_audio = np.copy(audio)\n"," for i in range(1, len(audio)) ：\n"," smoothed_audio[i] = smoothed_audio[i-1] * transition_prob[states[i]]\n","\n"," return smoothed_audio\n","# 示例音频\n","filename = 'your_audio_file.wav'\n","audio, sr = librosa.load(filename, sr=None)\n","# 设置HMM模型的状态数和状态转移概率，示例中简化为常数\n","n_states = 5\n","transition_prob = np.array([0.95, 0.9, 0.85, 0.9, 0.95])\n","# 进行状态转移平滑\n","smoothed_audio = smooth_with_hmm(audio, n_states, transition_prob)\n","# 保存合成的声音\n","output_filename = 'smoothed_audio_hmm.wav'\n","librosa.output.write_wav(output_filename, smoothed_audio, sr)"],"metadata":{"id":"FpfYkiHVErYq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3．协变量回归\n","#（2）实现代码\n","import numpy as np\n","import librosa\n","def smooth_with_covariates(audio, covariates) ：\n"," smoothed_audio = np.copy(audio)\n","\n"," # 根据协变量信息进行声音合成调整\n"," for i in range(1, len(audio)) ：\n"," smoothed_audio[i] = smoothed_audio[i-1] * covariates[i]\n","\n"," return smoothed_audio\n","# 示例音频\n","filename = 'your_audio_file.wav'\n","audio, sr = librosa.load(filename, sr=None)\n","# 示例协变量，示例中协变量简化为线性变化\n","covariates = np.linspace(0.8, 1.2, len(audio))\n","# 进行声音合成调整\n","smoothed_audio = smooth_with_covariates(audio, covariates)\n","# 保存合成的声音\n","output_filename = 'smoothed_audio_covariates.wav'\n","librosa.output.write_wav(output_filename, smoothed_audio, sr)"],"metadata":{"id":"7kqTcs81E1et"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.3　基于深度学习的语音合成\n","#4.3.1　LSTM 在语音合成中的应用\n","#2．LSTM 带来的优势\n","#（3）代码实现\n","#基于 LSTM 的神经网络语音合成系统的 Python 实现代码如下。\n","import numpy as np\n","import librosa\n","from keras.layers import LSTM, Dense\n","from keras.models import Sequential\n","# 载入语音样本并提取MFCC特征\n","audio, sr = librosa.load(\"speech.wav\", sr=16000)\n","mfcc = librosa.feature.mfcc(audio, sr=sr)\n","# 构建LSTM编码器-解码器模型\n","model = Sequential()\n","model.add(LSTM(128, input_shape=(None, 20), return_sequences=True)) # 编码器\n","model.add(LSTM(128, return_sequences=True)) # 解码器\n","model.add(Dense(20, activation='sigmoid')) # 输出层\n","# 训练模型参数\n","model.compile(loss='mse', optimizer='adam')\n","model.fit(mfccs, mfccs, epochs=10)\n","# 预测语音参数\n","mfcc_pred = model.predict(mfccs)\n","# 通过GL算法合成语音波形\n","audio_pred = librosa.griffinlim(mfcc_pred, n_iter=30)"],"metadata":{"id":"zE0POuRLE_4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.3.2　基于注意力机制的 Tacotron 模型\n","#2．模型训练过程\n","#（1）文本特征处理\n","import numpy as np\n","chars = \"this is some text\"\n","char_indices = dict((c, i) for i, c in enumerate(set(chars)))\n","indices = [char_indices[c] for c in chars]\n","embedding_dim = 20\n","embedding_matrix = np.random.randn(len(char_indices), embedding_dim)\n","char_embeds = embedding_matrix[indices]\n","\n","#（2）音频特征提取\n","#以下是音频特征提取代码，这里主要使用第三方库 librosa 来提取 MFCC 特征。\n","import librosa\n","def get_spectrograms(sound_file):\n"," # 加载声音文件\n"," y, sr = librosa.load(sound_file, sr=hp.sr) # or set sr to hp.sr.\n"," # 短时傅里叶变换\n"," D = librosa.stft(y=y,\n"," n_fft=hp.n_fft,\n"," hop_length=hp.hop_length,\n"," win_length=hp.win_length)\n"," # 幅度谱图\n"," magnitude = np.abs(D)\n"," # 功率谱图\n"," power = magnitude**2\n"," # 梅尔谱图\n"," S = librosa.feature.melspectrogram(S=power, n_mels=hp.n_mels)\n"," return np.transpose(S.astype(np.float32)), np.transpose(magnitude.astype(np.float32))\n","\n","#（3）端到端监督训练\n","from keras.layers import Attention, Dense, LSTM\n","from keras.models import Model\n","encoder = LSTM(...) # 编码器\n","decoder = LSTM(...) # 解码器\n","attn = Attention(...) # 注意力层\n","model = Model([encoder, decoder, attn], Dense(n_linear))\n","model.compile(loss='mse', ...)\n","model.fit([char_embeds, linear_spect], linear_spect, ...) # 端到端训练\n","\n","linear_pred = model.predict(char_embeds)"],"metadata":{"id":"sPZry0c_FXUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.3.3　Tacotron2 与 WaveNet 集成\n","#1．Tacotron2 的改进之处\n","#（3）代码实现\n","#Tacotron2 语音合成的 Python 代码实现示例如下，其中包含了 WaveNet 作为声码器与Tacotron2 结构优化的部分。\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","# Tacotron2编码器实现\n","input_chars = tf.keras.Input(shape=(None,))\n","char_embeddings = layers.Embedding(vocab_size, embedding_dim)(input_chars)\n","enc = layers.Conv1D(filters, kernel_size, activation='relu')(char_embeddings)\n","enc = layers.Bidirectional(layers.GRU(units, return_sequences=True))(enc)\n","# Tacotron2解码器实现\n","dec = layers.Conv1D(filters, kernel_size, activation='relu')(enc_output)\n","dec = layers.GRU(units, return_sequences=True)(dec, initial_state=enc_state)\n","attention = layers.BahdanauAttention()(dec, enc)\n","context = layers.Concatenate()([attention, dec])\n","# Tacotron2输出实现\n","decoder = layers.Conv1D(filters, kernel_size)(context)\n","mel_output = layers.Dense(mel_dim)(decoder)\n","# WaveNet声码器实现\n","wavenet = WaveNet(mel_input=mel_output, conditional_inputs=(...))\n","# 定义Tacotron2模型\n","model = tf.keras.Model(input_chars, wavenet.output)\n","# 编译与训练\n","model.compile(...)\n","model.fit(...)"],"metadata":{"id":"zXnXe-gMGEAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.4　语音风格迁移\n","#4.4.2　风格转换\n","#2．特征融合\n","#这里给出一个使用 VAE 进行语音风格转换中的特征融合的 Python 代码示例。\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# 定义VAE模型\n","class VAE(nn.Module) :\n"," def __init__(self, input_dim, latent_dim) :\n","  super(VAE, self).__init__()\n","\n","  # 编码器\n","  self.enc_fc1 = nn.Linear(input_dim, 512)\n","  self.enc_fc2 = nn.Linear(512, latent_dim*2)\n","\n","  # 解码器\n","  self.dec_fc1 = nn.Linear(latent_dim, 512)\n","  self.dec_fc2 = nn.Linear(512, input_dim)\n","\n"," def encode(self, x) :\n","  h = F.relu(self.enc_fc1(x))\n","  mu_logvar = self.enc_fc2(h).chunk(2, dim=1)\n","  return mu_logvar\n","\n"," def reparameterize(self, mu, logvar) :\n","  std = torch.exp(logvar/2)\n","  eps = torch.randn_like(std)\n","  return mu + eps * std\n","\n"," def decode(self, z) :\n","  h = F.relu(self.dec_fc1(z))\n","  recon_x = F.sigmoid(self.dec_fc2(h))\n","  return recon_x\n","\n"," def forward(self, x) :\n","  mu, logvar = self.encode(x)\n","  z = self.reparameterize(mu, logvar)\n","  recon_x = self.decode(z)\n","  return recon_x, mu, logvar\n","\n","# 输入的源语音内容特征和目标风格特征\n","content_fea = torch.randn(32, 256)\n","style_fea = torch.randn(32, 256)\n","# 编码内容，获得内容的分布参数mu和logvar\n","content_mu, content_logvar = vae.encode(content_fea)\n","# 对风格特征进行重参数化，得到风格码\n","style_std = torch.exp(style_logvar/2)\n","style_eps = torch.randn_like(style_std)\n","style_code = style_mu + style_eps * style_std\n","# 融合内容码和风格码\n","fused_code = content_mu + style_code\n","# 将融合码解码为新的语音特征\n","fused_fea = vae.decode(fused_code)"],"metadata":{"id":"kbcvKqBtGac5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.4.3　个性化语音合成\n","#2．多说话人模型\n","#这里给出一个使用说话人条件来实现多说话人语音合成的 Python 代码示例。\n","import torch\n","import torch.nn as nn\n","# 定义多说话人Tacotron模型\n","class MultiSpeakerTacotron(nn.Module) ：\n"," def __init__(self) ：\n","  super().__init__()\n","\n","  # 文本编码器\n","  self.text_encoder = TextEncoder()\n","\n","  # 声学特征解码器\n","  self.decoder = AcousticDecoder()\n","\n","  # 嵌入层，获得说话人条件嵌入向量\n","  self.spk_emb = nn.Embedding(num_speakers, spk_emb_dim)\n","\n"," def forward(self, text, spk_id) ：\n","  # 对文本进行编码\n","  text_fea = self.text_encoder(text)\n","\n","  # 获取说话人嵌入向量\n","  spk_emb = self.spk_emb(spk_id)\n","\n","  # 将文本特征和说话人嵌入向量拼接\n","  cond_input = torch.cat([text_fea, spk_emb], dim=-1)\n","\n","  # 解码得到声学特征\n","  mel_spect = self.decoder(cond_input)\n","\n","  return mel_spect\n","\n","# 实例化多说话人模型\n","model = MultiSpeakerTacotron()\n","# 输入文本序列\n","text = \"This is an example.\"\n","# 输入说话人ID，比如0、1、2等\n","spk_id = torch.LongTensor([1])\n","# 预测对应的语音特征\n","mel = model(text, spk_id)"],"metadata":{"id":"S8nCfl5THOoV"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/empty.ipynb","timestamp":1723357412101}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}